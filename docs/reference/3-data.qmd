---
title: Data
output-file: data
order: 3
toc: true
fig-align: center
---

BPMN uses *Data Object* and *Data Store* to represent elements that contain or manage data. These elements can be connected to other processes as inputs or outputs using data association edges.

Studyflow extends this by introducing elements designed to model data processing. This includes *`DataCatalog`* as well as elements such as `Map`, `Filter`, `Reduce`, `Group`, etc.

These extra elements facilitate the representation of data-centric workflows, particularly in research contexts.

While experimental data is generally assumed to be in a tabular format, these elements also support other data types, including images, videos, brain imaging, and raw sensor recordings.

Below is a summary of the data-related elements available in BPMN and Studyflow extensions.


## BPMN data elements

- `DataStore`: Persistent storage of data that can be accessed across multiple process instances. For example, a database or file system. `DataStoreReference` can be used as a pointer to a `DataStore`.
- `DataObject`: Available for the duration of a process instance. Multiple data objects can be used within a single process instance to simplify associations. BPMN data state annotations can be used to show the condition of a data object at a specific point in a process; for example, "trial data [raw]", "trial data [processed]".
- `DataAssociation`: Defines a link between data and other workflow elements, such as tasks or events. Note that it is not part of the process flow itself.

## Studyflow elements

- `DataCatalog`: A persistent repository of datasets that can be referenced across multiple process instances. For example, `openneuro` or `behaverse` catalogs.
- `Dataset`: A named logical collection (possibly multi-table, multi-modal).  Additional attributes include a schema (column names, types, units) and optional ontology to connect elements to standard vocabularies.
- `Table` (or `DataFrame`): A named tabular structure within a `Dataset`. Tables are explicitly linked to a schema via the `Dataset` or individually via CSVW linkage.
- `Snapshot`: An immutable version of a dataset or table.

In summary, `DataStore` is a physical/persistent store (database, filesystem, S3 bucket, etc.), `DataCatalog` is a registry of datasets (potentially across multiple stores), `Dataset` is a logical collection, and `Table` is a concrete tabular component of a dataset.

### Data operators

The data operators are inspired by functional programming concepts and are designed to facilitate data processing within workflows. They can be used to define how data is manipulated as it flows through the process.

data operators are rendered as specialized tasks in BPMN diagrams.

- `Transform`: Applies a specified transformation to the input data, producing a new dataset as output. This is the generic form of data operations. Each `Transform` node produces a new dataset with explicit references to its inputs for reproducible lineage tracking.
    - `TransformTables`: Special case of `Transform` that applies a series of transformations to tabular data, such as adding, removing, or modifying columns. The result is one or more new tables based on the specified transformations (1+ tables -> 1+ tables).
- `Map`: Applies a specified operation to each item in a dataset, producing a new dataset with the results. Extends `Transform` for element-wise operations (row-wise 1 -> 1).
- `Filter`: Selects a subset of data based on specified criteria, producing a new dataset containing only the items that meet those criteria. Extends `Transform` for conditional selection (1 -> subset(1)). `Filter` changes the dataset, but BPMN data-driven gateways change the control flow. They are complementary.
- `Reduce`: Aggregates data by applying a function that combines multiple input values into a single output value. Extends `Transform` for summarization or joining operations (N -> 1 per group or for the entire dataset).
- `FlatMap`: Similar to Map, but flattens the resulting datasets into a single dataset. Extends `Transform` for one-to-many mappings (1 -> N).
- `Group`: Organizes data into groups based on specified attributes. Extends `Transform` for categorization (1 -> N).

More complex operations (e.g., join, concatenate, window, resample, pivot) can be constructed by combining these basic functional elements within a workflow, allowing for more sophisticated data pipelines.

:::{.callout-tip appearance="simple"}
Note that data operations are treated as immutable. Each operation yields one or more new datasets.
:::


:::{.callout-warning appearance="simple"}
Some operators are stateless (Map, Filter) and works best for batch processing, while others are inherently stateful (Reduce, Group) and may require special handling for streaming data.
:::


### Planned elements
- `LoadData`, `SaveData`, `ExportData`: storage operations (loading from and saving to catalogs, stores, and files). Note that, data operations are pure and side-effect free. I/O and external systems are handled by dedicated elements.
- `AnonymizeData`, `ValidateData`, `ControlAccess`: data governance and regulatory compliance operations (de-identification, validation, data cleaning, and access control).
- stochastic operations, data split, `CleanData`, join/merge as canonical data operations.


## Example

The following example illustrates the use of data elements and operators within a research workflow to collect and analyze response times from a 2AFC cognitive task. The data analysis pipeline includes several data processing steps, all encapsulated within a subprocess for clarity.

```
Study RTAnalysis

  StartEvent s
  EndEvent e

  DataCatalog behaverse
    url "https://behaverse.org/catalog"

  DataStore ducklake
    kind "duckdb+parquet"
    url "s3://behaverse/rt/ducklake"

  Dataset study_dataset
    catalog behaverse
    store ducklake

  Table trials_raw
    dataset study_dataset
    schema "schema/trials_raw.csvw"

  Table trials_summary
    dataset study_dataset
    schema "schema/trials_summary.csvw"

  Activity CollectTrials
    @type CognitiveTest
    description "Run 2AFC task"

  SubProcess RTAnalysisPipeline
    StartEvent sub_s
    EndEvent sub_e

    # internal data objects (scoped to subprocess)
    DataObject trials_in
    DataObject trials_out

    # outer data associations: connect external tables to internal nodes
    dataInputAssociation
      sourceRef trials_raw          # external
      targetRef trials_in           # internal name
    dataOutputAssociation
      sourceRef trials_out          # internal name
      targetRef trials_summary      # external

    Task t1
      @in trials_in
      @out trials_out
      @op Compose
        TransformTables
        Filter
        Map
        Group
        Reduce

    SequenceFlow sf1 sub_s → t1
    SequenceFlow sf2 t1    → sub_e

  SequenceFlow f1 s                    → CollectTrials
  SequenceFlow f2 CollectTrials        → RTAnalysisPipeline
  SequenceFlow f3 RTAnalysisPipeline   → e
  ```
